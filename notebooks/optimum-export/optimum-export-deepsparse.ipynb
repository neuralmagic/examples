{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting Hugging Face Models Using Optimum and Running Them in DeepSparse\n",
    "\n",
    "This guide harnesses the power of Neural Magic's DeepSparse Inference Runtime library in combination with Hugging Face's ONNX models. DeepSparse offers a cutting-edge solution for efficient and accelerated inference on deep learning models, optimizing performance and resource utilization. By seamlessly integrating DeepSparse with Hugging Face's ONNX models, users can experience lightning-fast inference times while maintaining the flexibility and versatility of the widely adopted ONNX format alongside the  `Optimum` library for PyTorch model ONNX exporting.\n",
    "\n",
    "This notebook will use several popular models found on the Hugging Face Hub for text classification, zero-shot classification, question answering, and NER.\n",
    "\n",
    "The flow for this guide includes:\n",
    "\n",
    "1. Exporting models to ONNX using `optimum-cli`.\n",
    "2. Running inference with ONNX models with DeepSparse.\n",
    "\n",
    "## Install DeepSparse and Optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install deepsparse-nightly[transformers] optimum[exporters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification | Sentiment Analysis\n",
    "\n",
    "Let's export the DistilBERT SST-2 model for sentiment analysis to an output folder called `tc_model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!optimum-cli export onnx --model distilbert-base-uncased-finetuned-sst-2-english tc_model --sequence_length 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model and run inference with DeepSparse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "from deepsparse import Pipeline\n",
    "\n",
    "def run_deepsparse(model_path, text_input, task):\n",
    "    pipeline = Pipeline.create(task=task, model_path=model_path)\n",
    "    start_time = perf_counter()\n",
    "    inference = pipeline(text_input)\n",
    "    end_time = perf_counter()\n",
    "    execution_time_deepsparse = end_time - start_time\n",
    "    return inference, execution_time_deepsparse\n",
    "\n",
    "model_path = \"./tc_model\"\n",
    "task = \"sentiment-analysis\"\n",
    "text_input = \"Snorlax loves my Tesla!\"\n",
    "\n",
    "inference_deepsparse, execution_time_deepsparse = run_deepsparse(model_path, text_input, task)\n",
    "print(f\"Deepsparse code snippet execution time: {execution_time_deepsparse:.4f} seconds\")\n",
    "print(inference_deepsparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER\n",
    "\n",
    "Let's export the BERT Base NER model to an output folder called `ner_model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!optimum-cli export onnx --model dslim/bert-base-NER ner_model --sequence_length 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model and run inference with DeepSparse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "from deepsparse import Pipeline\n",
    "\n",
    "def run_deepsparse(model_path, text_input, task):\n",
    "    pipeline = Pipeline.create(task=task, model_path=model_path)\n",
    "    start_time = perf_counter()\n",
    "    inference = pipeline(text_input)\n",
    "    end_time = perf_counter()\n",
    "    execution_time_deepsparse = end_time - start_time\n",
    "    return inference, execution_time_deepsparse\n",
    "\n",
    "model_path = \"./ner_model/model.onnx\"\n",
    "task = \"token-classification\"\n",
    "text_input = \"Snorlax loves my Tesla!\"\n",
    "\n",
    "inference_deepsparse, execution_time_deepsparse = run_deepsparse(model_path, text_input, task)\n",
    "print(f\"Deepsparse code snippet execution time: {execution_time_deepsparse:.4f} seconds\")\n",
    "print(inference_deepsparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering\n",
    "\n",
    "Let's export the RoBERTa Base model for Question Answering to an output folder called `qa_model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!optimum-cli export onnx --model deepset/roberta-base-squad2 qa_model --sequence_length 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model and run inference with DeepSparse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "from deepsparse import Pipeline\n",
    "\n",
    "def run_deepsparse(model_path, question, context, task):\n",
    "    pipeline = Pipeline.create(task=task, model_path=model_path)\n",
    "    start_time = perf_counter()\n",
    "    inference = pipeline(question=\"What's my name?\", context=\"My name is Snorlax\")\n",
    "    end_time = perf_counter()\n",
    "    execution_time_deepsparse = end_time - start_time\n",
    "    return inference, execution_time_deepsparse\n",
    "\n",
    "\n",
    "model_path = \"./qa_model/model.onnx\"\n",
    "task = \"question-answering\"\n",
    "question = \"who loves Tesla?\"\n",
    "context = \"Snorlax loves my Tesla?\"\n",
    "\n",
    "inference_deepsparse, execution_time_deepsparse = run_deepsparse(model_path, question, context, task)\n",
    "print(f\"Deepsparse code snippet execution time: {execution_time_deepsparse:.4f} seconds\")\n",
    "print(inference_deepsparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot Text Classification\n",
    "\n",
    "Let's export the DistilBERT MNLI Base model to an output folder called `zs_model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!optimum-cli export onnx --model typeform/distilbert-base-uncased-mnli zs_model --sequence_length 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model and run inference with DeepSparse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "from deepsparse import Pipeline\n",
    "\n",
    "def run_deepsparse(model_path, text_input, task, labels):\n",
    "    pipeline = Pipeline.create(\n",
    "        task=task, model_scheme=\"mnli\", \n",
    "        model_config={\"hypothesis_template\": \"This text is related to {}\"}, \n",
    "        model_path=model_path\n",
    "    )\n",
    "    start_time = perf_counter()\n",
    "    inference = pipeline(text_input, labels)\n",
    "    end_time = perf_counter()\n",
    "    execution_time_deepsparse = end_time - start_time\n",
    "    return inference, execution_time_deepsparse\n",
    "\n",
    "model_path = \"./zs_model/model.onnx\"\n",
    "task = \"zero_shot_text_classification\"\n",
    "text_input = \"I like pepperoni pizza.\"\n",
    "labels = [\"food\", \"movies\", \"sports\"]\n",
    "\n",
    "inference_deepsparse, execution_time_deepsparse = run_deepsparse(model_path, text_input, task, labels)\n",
    "print(f\"Deepsparse code snippet execution time: {execution_time_deepsparse:.4f} seconds\")\n",
    "print(inference_deepsparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification\n",
    "\n",
    "Let's export the Resnet-50 model to an output folder called `ic_model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!optimum-cli export onnx --model microsoft/resnet-50 ic_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model and run inference with DeepSparse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "from deepsparse import Pipeline\n",
    "\n",
    "def run_deepsparse(model_path, image, task):\n",
    "    pipeline = Pipeline.create(task=task, model_path=model_path, input_shapes=[1,3,224,224])\n",
    "    start_time = perf_counter()\n",
    "    inference = pipeline(images=image)\n",
    "    end_time = perf_counter()\n",
    "    execution_time_deepsparse = end_time - start_time\n",
    "    return inference, execution_time_deepsparse\n",
    "\n",
    "image = \"./notebooks/optimum-export/cat.jpg\"\n",
    "model_path =\"ic_model/model.onnx\"\n",
    "task = \"image_classification\"\n",
    "\n",
    "inference_deepsparse, execution_time_deepsparse = run_deepsparse(model_path, image, task)\n",
    "print(f\"Deepsparse code snippet execution time: {execution_time_deepsparse:.4f} seconds\")\n",
    "print(inference_deepsparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Segmentation\n",
    "\n",
    "Let's export the DEtection TRansformer(DETR) model to an output folder called `is_model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!optimum-cli export onnx --model facebook/detr-resnet-50-panoptic is_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model and run inference with DeepSparse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "from deepsparse import Pipeline\n",
    "\n",
    "def run_deepsparse(model_path, image, task):\n",
    "    pipeline = Pipeline.create(task=task, model_path=model_path, input_shapes=[1,3,224,224], image_size=(224,224))\n",
    "    start_time = perf_counter()\n",
    "    inference = pipeline(images=image)\n",
    "    end_time = perf_counter()\n",
    "    execution_time_deepsparse = end_time - start_time\n",
    "    return inference, execution_time_deepsparse\n",
    "\n",
    "image = \"./notebooks/optimum-export/thailand.jpeg\"\n",
    "model_path =\"is_model/model.onnx\"\n",
    "task = \"yolov8\"\n",
    "\n",
    "inference_deepsparse, execution_time_deepsparse = run_deepsparse(model_path, image, task)\n",
    "print(f\"Deepsparse code snippet execution time: {execution_time_deepsparse:.4f} seconds\")\n",
    "print(inference_deepsparse)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
