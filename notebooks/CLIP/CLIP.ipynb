{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CLIP Inference Pipelines With DeepSparse\n",
        ""
      ],
      "metadata": {
        "id": "qUErSlAUYIK1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[CLIP](https://github.com/mlfoundations/open_clip/tree/main) models can be used for zero-shot image classification and generating captions given an image. This notebook illustrates how to perform these two tasks on a CPU using CLIP and [DeepSparse](https://github.com/neuralmagic/deepsparse)."
      ],
      "metadata": {
        "id": "6LlBVf0oZiXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run this notebook succesfully, you need to install `open_clip_torch==2.20.0`. This can be achieved by installing `sparseml[clip]`. Other required installations are:\n",
        "\n",
        "- `deepsparse[clip]`\n",
        "- `torch-nightly`"
      ],
      "metadata": {
        "id": "j1Avjnbxa_hz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Etxmn1BtHg0U"
      },
      "outputs": [],
      "source": [
        "pip install sparseml-nightly[clip]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set this to today's torch nightly version\n",
        "import os\n",
        "os.environ[\"MAX_TORCH\"] = \"2.2.0.dev20230911+cpu\""
      ],
      "metadata": {
        "id": "M6XroPYgi0xV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2N0YYFiG11Zs"
      },
      "outputs": [],
      "source": [
        "pip install deepsparse-nightly[clip]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5isE8W1XRnk"
      },
      "outputs": [],
      "source": [
        "pip uninstall -y  torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0IjClMiuYDF"
      },
      "outputs": [],
      "source": [
        "pip install --pre torch --index-url https://download.pytorch.org/whl/nightly/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Test Images"
      ],
      "metadata": {
        "id": "ka_7r1M0cslf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaYwxfCD2ObY"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "wget -O basilica.jpg https://raw.githubusercontent.com/neuralmagic/deepsparse/main/src/deepsparse/yolo/sample_images/basilica.jpg\n",
        "wget -O buddy.jpeg https://raw.githubusercontent.com/neuralmagic/deepsparse/main/tests/deepsparse/pipelines/sample_images/buddy.jpeg\n",
        "wget -O thailand.jpg https://raw.githubusercontent.com/neuralmagic/deepsparse/main/src/deepsparse/yolact/sample_images/thailand.jpg"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![basilica](https://raw.githubusercontent.com/neuralmagic/deepsparse/main/src/deepsparse/yolo/sample_images/basilica.jpg\n",
        ")\n",
        "![dog](https://raw.githubusercontent.com/neuralmagic/deepsparse/main/tests/deepsparse/pipelines/sample_images/buddy.jpeg\n",
        ")\n",
        "![dog](https://raw.githubusercontent.com/neuralmagic/deepsparse/main/src/deepsparse/yolact/sample_images/thailand.jpg\n",
        ")"
      ],
      "metadata": {
        "id": "LgQBc9CTj2oC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPCCcereIqfk"
      },
      "source": [
        "## Zero-shot Image Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You need to provide the CLIP models in the ONNX format to DeepSparse. You can obtain these fikes by exporting the CLIP models using the provided export scripts."
      ],
      "metadata": {
        "id": "HqBdDp8adAAQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-_q8_cm4cmK"
      },
      "source": [
        "### Export Models Using SparseML"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, download the export scripts:"
      ],
      "metadata": {
        "id": "KHGOm_7wdQEE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHVZ02DMFPaW"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "wget https://raw.githubusercontent.com/neuralmagic/sparseml/main/integrations/clip/clip_models.py\n",
        "wget https://raw.githubusercontent.com/neuralmagic/sparseml/main/integrations/clip/clip_onnx_export.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Export the CLIP models for zero-shot classification.\n",
        "\n",
        "The pre-trained models can be found on the [OpenCIP GitHub repository](https://github.com/mlfoundations/open_clip/tree/main).\n",
        "\n",
        "\n",
        "Running the export script exports a visual model and a text model which are then passed to the DeepSparse Pipeline for inference."
      ],
      "metadata": {
        "id": "BhvjpfKsdU81"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqDrrDlCFPOt",
        "outputId": "0d42a3c8-a416-4a3f-b985-3c83c0e2056e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/mwiti/.venv/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libc10_cuda.so: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n",
            "/home/ubuntu/mwiti/.venv/lib/python3.8/site-packages/torch/onnx/utils.py:823: UserWarning: It is recommended that constant folding be turned off ('do_constant_folding=False') when exporting the model in training-amenable mode, i.e. with 'training=TrainingMode.TRAIN' or 'training=TrainingMode.PRESERVE' (when model is in training mode). Otherwise, some learnable model parameters may not translate correctly in the exported ONNX model because constant folding mutates model parameters. Please consider turning off constant folding or setting the training=TrainingMode.EVAL.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/mwiti/.venv/lib/python3.8/site-packages/torch/onnx/symbolic_opset9.py:5856: UserWarning: Exporting aten::index operator of advanced indexing in opset 14 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "python clip_onnx_export.py --model convnext_base_w_320 \\\n",
        "            --pretrained laion_aesthetic_s13b_b82k --export-path convnext_onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform zero-shot image classification using the CLIP models and DeepSparse by providing the images, possible classes and the path to the CLIP models while specifying the task as `clip_zeroshot`."
      ],
      "metadata": {
        "id": "fh7Jt427dsew"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JK8JIOw52hk_",
        "outputId": "d9e46528-fd7c-4e8f-8505-7cd0e637fab1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/mwiti/.venv/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libc10_cuda.so: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n",
            "DeepSparse, Copyright 2021-present / Neuralmagic, Inc. version: 1.6.0.20230906 COMMUNITY | (f5e597bf) (release) (optimized) (system=avx2, binary=avx2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image basilica.jpg is a picture of a church\n",
            "Image buddy.jpeg is a picture of a dog\n",
            "Image thailand.jpg is a picture of an elephant\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "from deepsparse import BasePipeline\n",
        "from deepsparse.clip import (\n",
        "    CLIPTextInput,\n",
        "    CLIPVisualInput,\n",
        "    CLIPZeroShotInput\n",
        ")\n",
        "\n",
        "possible_classes = [\"ice cream\", \"an elephant\", \"a dog\", \"a building\", \"a church\"]\n",
        "images = [\"basilica.jpg\", \"buddy.jpeg\", \"thailand.jpg\"]\n",
        "\n",
        "model_path_text = \"convnext_onnx/clip_text.onnx\"\n",
        "model_path_visual = \"convnext_onnx/clip_visual.onnx\"\n",
        "\n",
        "kwargs = {\n",
        "    \"visual_model_path\": model_path_visual,\n",
        "    \"text_model_path\": model_path_text,\n",
        "}\n",
        "pipeline = BasePipeline.create(task=\"clip_zeroshot\", **kwargs)\n",
        "\n",
        "pipeline_input = CLIPZeroShotInput(\n",
        "    image=CLIPVisualInput(images=images),\n",
        "    text=CLIPTextInput(text=possible_classes),\n",
        ")\n",
        "\n",
        "output = pipeline(pipeline_input).text_scores\n",
        "for i in range(len(output)):\n",
        "    prediction = possible_classes[np.argmax(output[i])]\n",
        "    print(f\"Image {images[i]} is a picture of {prediction}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HA438m52m36"
      },
      "source": [
        "## Image Caption Generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image caption generation can be done in a similar manner as zero-shot image classification."
      ],
      "metadata": {
        "id": "2l5dctZPhogI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpCbHMy-IwVC"
      },
      "source": [
        "### Export Models Using SparseML"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step is to export the CLIP models. The provided script will export the visual, text and text endoder models.\n"
      ],
      "metadata": {
        "id": "lGrVzxTOh3-f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNKmuSCjG22i",
        "outputId": "e566e914-e3e6-4122-a989-de457bf02852"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/mwiti/.venv/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libc10_cuda.so: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n",
            "Downloading (…)ip_pytorch_model.bin: 100%|██████████| 1.01G/1.01G [01:25<00:00, 11.8MB/s]\n",
            "/home/ubuntu/mwiti/.venv/lib/python3.8/site-packages/torch/onnx/utils.py:823: UserWarning: It is recommended that constant folding be turned off ('do_constant_folding=False') when exporting the model in training-amenable mode, i.e. with 'training=TrainingMode.TRAIN' or 'training=TrainingMode.PRESERVE' (when model is in training mode). Otherwise, some learnable model parameters may not translate correctly in the exported ONNX model because constant folding mutates model parameters. Please consider turning off constant folding or setting the training=TrainingMode.EVAL.\n",
            "  warnings.warn(\n",
            "In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "python clip_onnx_export.py --model coca_ViT-B-32 \\\n",
        "            --pretrained mscoco_finetuned_laion2b_s13b_b90k --export-path caption_models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, run inference by providing the path of the downloaded CLIP models to the DeepSparse Pipeline while specifying the task as `clip_caption`. Then specify the image you'd like to run inference on."
      ],
      "metadata": {
        "id": "qZMGt_vih8vo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fr9IRaHE2mYa",
        "outputId": "1fb7d348-437e-41a2-e30a-6f13ad8214c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "an adult elephant and a baby elephant \n"
          ]
        }
      ],
      "source": [
        "from deepsparse import BasePipeline\n",
        "from deepsparse.clip import CLIPCaptionInput, CLIPVisualInput\n",
        "\n",
        "root = \"caption_models\"\n",
        "model_path_visual = f\"{root}/clip_visual.onnx\"\n",
        "model_path_text = f\"{root}/clip_text.onnx\"\n",
        "model_path_decoder = f\"{root}/clip_text_decoder.onnx\"\n",
        "engine_args = {\"num_cores\": 8}\n",
        "\n",
        "kwargs = {\n",
        "    \"visual_model_path\": model_path_visual,\n",
        "    \"text_model_path\": model_path_text,\n",
        "    \"decoder_model_path\": model_path_decoder,\n",
        "    \"pipeline_engine_args\": engine_args\n",
        "}\n",
        "pipeline = BasePipeline.create(task=\"clip_caption\", **kwargs)\n",
        "\n",
        "pipeline_input = CLIPCaptionInput(image=CLIPVisualInput(images=\"thailand.jpg\"))\n",
        "output = pipeline(pipeline_input).caption\n",
        "print(output[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uUoCoHUOjtD",
        "outputId": "893e4fc1-767a-4ea4-e77e-4471534b3eb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a close up of the dog 's mouth is very happy \n"
          ]
        }
      ],
      "source": [
        "from deepsparse import BasePipeline\n",
        "from deepsparse.clip import CLIPCaptionInput, CLIPVisualInput\n",
        "\n",
        "root = \"caption_models\"\n",
        "model_path_visual = f\"{root}/clip_visual.onnx\"\n",
        "model_path_text = f\"{root}/clip_text.onnx\"\n",
        "model_path_decoder = f\"{root}/clip_text_decoder.onnx\"\n",
        "engine_args = {\"num_cores\": 8}\n",
        "\n",
        "kwargs = {\n",
        "    \"visual_model_path\": model_path_visual,\n",
        "    \"text_model_path\": model_path_text,\n",
        "    \"decoder_model_path\": model_path_decoder,\n",
        "    \"pipeline_engine_args\": engine_args\n",
        "}\n",
        "pipeline = BasePipeline.create(task=\"clip_caption\", **kwargs)\n",
        "\n",
        "pipeline_input = CLIPCaptionInput(image=CLIPVisualInput(images=\"buddy.jpeg\"))\n",
        "output = pipeline(pipeline_input).caption\n",
        "print(output[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Where to go From Here\n",
        "\n",
        "Join us on [Slack](https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ) for any questions or create an issue on [GitHub](https://github.com/neuralmagic)."
      ],
      "metadata": {
        "id": "KeEwpG5ekOeN"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}