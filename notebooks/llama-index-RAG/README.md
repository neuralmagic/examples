# Integrating a Sparse MiniLM in a RAG Architecture For PDF Retrieval

[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuralmagic/examples/blob/main/notebooks/llama-index-RAG/TinyLlama_Llama_Index_RAG.ipynb
)

In this Colab notebook, we integrate a Sparse MiniLM running with DeepSparse in a Retrieval-Augmented Generation (RAG) architecture with the intent of generating embeddings. This example aims to clarify the process of integrating a Sparse sentence embedding pipeline for faster and more efficient embedding generation while using developer friendly stacks such as llama-index for resource orchestration.